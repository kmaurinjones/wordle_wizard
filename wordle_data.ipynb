{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request # for downloading if necessary\n",
    "import operator # for sorting letter frequency distribution\n",
    "from nltk.corpus import movie_reviews, treebank, brown, gutenberg, switchboard\n",
    "from wordle_functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Datasets\n",
    "- Get all possible words that the target word could be\n",
    "- For each word in the target words list, get counts of each letter to create letter distribution across entire vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `alt_words_1` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bleat',\n",
       " 'zooea',\n",
       " 'talks',\n",
       " 'tango',\n",
       " 'kecks',\n",
       " 'maare',\n",
       " 'bolus',\n",
       " 'drats',\n",
       " 'grise',\n",
       " 'salas']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### If getting words from local file -- should be 14855 words in total\n",
    "\n",
    "alt_words_1 = set() # set of all words\n",
    "\n",
    "file_path = \"data/alt_words_1.txt\" # taken from \"https://raw.githubusercontent.com/tabatkins/wordle-list/main/words\"\n",
    "f = open(file_path, \"r\", encoding = \"utf-8\")\n",
    "\n",
    "with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "    for word in f.read().split(\"\\n\"):\n",
    "        if len(word) > 0: # there's one blank entry at the start\n",
    "            alt_words_1.add(word)\n",
    "\n",
    "f.close() # closes connection to file\n",
    "\n",
    "print(len(alt_words_1))\n",
    "alt_words_1 = list(alt_words_1)\n",
    "alt_words_1[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `official_words` list dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['foist',\n",
       " 'dowdy',\n",
       " 'bleat',\n",
       " 'basis',\n",
       " 'tango',\n",
       " 'eking',\n",
       " 'knead',\n",
       " 'power',\n",
       " 'dwell',\n",
       " 'bleep']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "official_words = set() # set of all words\n",
    "\n",
    "file_path = \"data/official_words_unprocessed.txt\"\n",
    "f = open(file_path, \"r\", encoding = \"utf-8\")\n",
    "\n",
    "with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "    for line in f.read().split(\"\\n\"):\n",
    "        word = line.split(\" \")[-1]\n",
    "        if (len(word) != 5 or word.isalpha() == False):\n",
    "            pass\n",
    "        else:\n",
    "            official_words.add(word.lower())\n",
    "\n",
    "f.close() # closes connection to file\n",
    "\n",
    "for word in official_words:\n",
    "    if len(word) != 5:\n",
    "        print (word)\n",
    "\n",
    "official_words = list(set(official_words))\n",
    "print(len(official_words))\n",
    "official_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### writing clean words list to .txt\n",
    "with open(\"data/official_words_processed.txt\", \"w\") as fout:\n",
    "    for word in official_words:\n",
    "        fout.write(word + \"\\n\")\n",
    "\n",
    "f.close() # closes connection to file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand Corpus Development\n",
    "- 2132 words in common with official wordle list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107314\n"
     ]
    }
   ],
   "source": [
    "brown_words_tokens = []\n",
    "\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            brown_words_tokens.append(word)\n",
    "\n",
    "print(len(brown_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072\n",
      "['hanch', 'basis', 'bleat', 'serra', 'upton', 'talks', 'holes', 'tango', 'dijon', 'shill']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_words_types = set(brown_words_tokens)\n",
    "print(len(brown_words_types))\n",
    "print((list(brown_words_types)[:10]))\n",
    "\n",
    "missing_words_brown = set()\n",
    "for word in official_words:\n",
    "    if word not in brown_words_types:\n",
    "        missing_words_brown.add(word)\n",
    "        \n",
    "len(missing_words_brown)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8586\n"
     ]
    }
   ],
   "source": [
    "treebank_words_tokens = []\n",
    "\n",
    "for word in treebank.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            treebank_words_tokens.append(word)\n",
    "\n",
    "print(len(treebank_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096\n",
      "['meets', 'basis', 'coast', 'issue', 'noble', 'talks', 'years', 'short', 'smaby', 'china']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1706"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_words_types = set(treebank_words_tokens)\n",
    "print(len(treebank_words_types))\n",
    "print((list(treebank_words_types)[:10]))\n",
    "\n",
    "missing_words_treebank = set()\n",
    "for word in official_words:\n",
    "    if word not in treebank_words_types:\n",
    "        missing_words_treebank.add(word)\n",
    "        \n",
    "len(missing_words_treebank)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### switchboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6393\n"
     ]
    }
   ],
   "source": [
    "switchboard_words_tokens = []\n",
    "\n",
    "for word in switchboard.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            switchboard_words_tokens.append(word)\n",
    "\n",
    "print(len(switchboard_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656\n",
      "['basis', 'meets', 'coast', 'issue', 'talks', 'years', 'short', 'china', 'latin', 'tokyo']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1864"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchboard_words_types = set(switchboard_words_tokens)\n",
    "print(len(switchboard_words_types))\n",
    "print((list(switchboard_words_types)[:10]))\n",
    "\n",
    "missing_words_switchboard = set()\n",
    "for word in official_words:\n",
    "    if word not in switchboard_words_types:\n",
    "        missing_words_switchboard.add(word)\n",
    "        \n",
    "len(missing_words_switchboard)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249307\n"
     ]
    }
   ],
   "source": [
    "gutenberg_words_tokens = []\n",
    "\n",
    "for word in gutenberg.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            gutenberg_words_tokens.append(word)\n",
    "\n",
    "print(len(gutenberg_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4684\n",
      "['basis', 'bleat', 'chiun', 'zolas', 'talks', 'padan', 'holes', 'gaine', 'kison', 'latin']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg_words_types = set(gutenberg_words_tokens)\n",
    "print(len(gutenberg_words_types))\n",
    "print((list(gutenberg_words_types)[:10]))\n",
    "\n",
    "missing_words_gutenberg = set()\n",
    "for word in official_words:\n",
    "    if word not in gutenberg_words_types:\n",
    "        missing_words_gutenberg.add(word)\n",
    "        \n",
    "len(missing_words_gutenberg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163588\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_words_tokens = []\n",
    "\n",
    "for word in movie_reviews.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            movie_reviews_words_tokens.append(word)\n",
    "\n",
    "print(len(movie_reviews_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4508\n",
      "['dowdy', 'basis', 'serra', 'holes', 'talks', 'tango', 'shill', 'sluts', 'latin', 'mooch']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews_words_types = set(movie_reviews_words_tokens)\n",
    "print(len(movie_reviews_words_types))\n",
    "print((list(movie_reviews_words_types)[:10]))\n",
    "\n",
    "missing_words_movie_reviews = set()\n",
    "for word in official_words:\n",
    "    if word not in movie_reviews_words_types:\n",
    "        missing_words_movie_reviews.add(word)\n",
    "        \n",
    "len(missing_words_movie_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grand Corpus (complete)\n",
    "- 8043 types\n",
    "- only 177 missing from official wordle list (2132 / 2109, 92% mutual)\n",
    "- written to text file and put in working directory (put this in another .ipynb called \"data processing\" or something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535188\n",
      "['years', 'board', 'dutch', 'group', 'agnew', 'years', 'named', 'among', 'group', 'years', 'fiber', 'lungs', 'brief', 'later', 'loews', 'makes', 'using', 'today', 'forum', 'bring']\n"
     ]
    }
   ],
   "source": [
    "all_corpora = [treebank_words_tokens, brown_words_tokens, gutenberg_words_tokens, switchboard_words_tokens, movie_reviews_words_tokens]\n",
    "grand_corpus_tokens = []\n",
    "for corpus in all_corpora:\n",
    "    for word in corpus:\n",
    "        grand_corpus_tokens.append(word)\n",
    "\n",
    "print(len(grand_corpus_tokens))\n",
    "print(grand_corpus_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### writing just words to .txt file\n",
    "with open(\"data/nltk_grand_corpus_tokens.txt\", \"w\") as fout:\n",
    "    for word in grand_corpus_tokens:\n",
    "        fout.write(word + \"\\n\")\n",
    "\n",
    "grand_corpus_word_freq = get_word_distribution(grand_corpus_tokens, sort = \"descending\")\n",
    "\n",
    "### writing words and counts to .txt file\n",
    "with open(\"data/nltk_grand_corpus_types_and_counts.txt\", \"w\") as fout:\n",
    "    for word, count in grand_corpus_word_freq:\n",
    "        fout.write(word + \"\\t\" + str(count) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8043\n",
      "['bleat', 'chiun', 'talks', 'tango', 'padan', 'tomas', 'aboue', 'fames', 'curre', 'leigh', 'sense', 'freed', 'buoys', 'fulci', 'react', 'belly', 'defoe', 'deify', 'rider', 'codes']\n",
      "177\n",
      "['foist', 'eking', 'bleep', 'latte', 'cabal', 'duvet', 'parer', 'mange', 'mucky', 'ombre', 'nomad', 'junto', 'femur', 'kneed', 'tabby', 'debit', 'betel', 'ovine', 'gamer', 'aphid']\n"
     ]
    }
   ],
   "source": [
    "grand_corpus_types = set(grand_corpus_tokens)\n",
    "print(len(grand_corpus_types))\n",
    "print(list(grand_corpus_types)[:20])\n",
    "\n",
    "grand_corpus_missing = []\n",
    "for word in official_words:\n",
    "    if word not in grand_corpus_types:\n",
    "        grand_corpus_missing.append(word)\n",
    "\n",
    "print(len(grand_corpus_missing))\n",
    "print(list(grand_corpus_missing)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most frequent words in grand corpus:\n",
      "\n",
      "[('which', 15760), ('their', 13925), ('there', 13447), ('shall', 11997), ('would', 9361), ('about', 8388), ('could', 6851), ('other', 6340), ('these', 5859), ('movie', 5826), ('after', 5686), ('first', 5294), ('great', 4738), ('where', 4403), ('every', 4398), ('never', 4030), ('house', 3960), ('being', 3789), ('those', 3657), ('while', 3534)]\n",
      "\n",
      "20 least frequent words in grand corpus:\n",
      "\n",
      "[('agnew', 1), ('borge', 1), ('menem', 1), ('imsai', 1), ('gingl', 1), ('harpo', 1), ('chary', 1), ('kuala', 1), ('shrum', 1), ('kelli', 1), ('nelms', 1), ('desai', 1), ('kuhns', 1), ('erode', 1), ('kuvin', 1), ('soups', 1), ('coors', 1), ('spiro', 1), ('milne', 1), ('rotie', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_freq_20 = get_word_distribution(grand_corpus_tokens, sort = \"descending\")[:20]\n",
    "least_freq_20 = get_word_distribution(grand_corpus_tokens, sort = \"ascending\")[:20]\n",
    "print(f\"20 most frequent words in grand corpus:\\n\\n{most_freq_20}\\n\")\n",
    "print(f\"20 least frequent words in grand corpus:\\n\\n{least_freq_20}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('which', 15760),\n",
       " ('their', 13925),\n",
       " ('there', 13447),\n",
       " ('shall', 11997),\n",
       " ('would', 9361),\n",
       " ('about', 8388),\n",
       " ('could', 6851),\n",
       " ('other', 6340),\n",
       " ('these', 5859),\n",
       " ('movie', 5826)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grand_freqs = get_word_distribution(grand_corpus_tokens, sort = \"descending\")\n",
    "grand_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132\n"
     ]
    }
   ],
   "source": [
    "wordle_freq_ratings = []\n",
    "for word in official_words:\n",
    "    for tup in grand_freqs:\n",
    "        if tup[0] == word:\n",
    "            wordle_freq_ratings.append(tup)\n",
    "\n",
    "found_words_sorted = sorted(wordle_freq_ratings, key = operator.itemgetter(1), reverse = True) # sorted descending\n",
    "print(len(found_words_sorted)) # 2132 of 2309 wordle words will have a frequency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
