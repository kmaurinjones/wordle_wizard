{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request # for downloading if necessary\n",
    "import operator # for sorting letter frequency distribution\n",
    "from nltk.corpus import movie_reviews, treebank, brown, gutenberg, switchboard\n",
    "from wordle_functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Datasets\n",
    "- Get all possible words that the target word could be\n",
    "- For each word in the target words list, get counts of each letter to create letter distribution across entire vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `alt_words_1` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['thyme',\n",
       " 'allin',\n",
       " 'judgy',\n",
       " 'serac',\n",
       " 'raggs',\n",
       " 'hauls',\n",
       " 'oliva',\n",
       " 'skail',\n",
       " 'buyin',\n",
       " 'imbos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### If getting words from local file -- should be 14855 words in total\n",
    "\n",
    "alt_words_1 = set() # set of all words\n",
    "\n",
    "file_path = \"data/alt_words_1.txt\" # taken from \"https://raw.githubusercontent.com/tabatkins/wordle-list/main/words\"\n",
    "\n",
    "with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "    for word in f.read().split(\"\\n\"):\n",
    "        if len(word) > 0: # there's one blank entry at the start\n",
    "            alt_words_1.add(word)\n",
    "\n",
    "f.close() # closes connection to file\n",
    "\n",
    "print(len(alt_words_1))\n",
    "alt_words_1 = list(alt_words_1)\n",
    "alt_words_1[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `official_words` list dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wince',\n",
       " 'thyme',\n",
       " 'mower',\n",
       " 'horde',\n",
       " 'heard',\n",
       " 'tenor',\n",
       " 'zonal',\n",
       " 'parry',\n",
       " 'shied',\n",
       " 'fizzy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "official_words = set() # set of all words\n",
    "\n",
    "file_path = \"data/official_words_unprocessed.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "    for line in f.read().split(\"\\n\"):\n",
    "        word = line.split(\" \")[-1]\n",
    "        if (len(word) != 5 or word.isalpha() == False):\n",
    "            pass\n",
    "        else:\n",
    "            official_words.add(word.lower())\n",
    "\n",
    "f.close() # closes connection to file\n",
    "\n",
    "for word in official_words:\n",
    "    if len(word) != 5:\n",
    "        print (word)\n",
    "\n",
    "official_words = list(set(official_words))\n",
    "print(len(official_words))\n",
    "official_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### writing clean words list to .txt\n",
    "with open(\"data/official_words_processed.txt\", \"w\") as fout:\n",
    "    for word in official_words:\n",
    "        fout.write(word + \"\\n\")\n",
    "\n",
    "f.close() # closes connection to file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand Corpus Development\n",
    "- 2132 words in common with official wordle list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107314\n"
     ]
    }
   ],
   "source": [
    "brown_words_tokens = []\n",
    "\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            brown_words_tokens.append(word)\n",
    "\n",
    "print(len(brown_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072\n",
      "['hoots', 'hauls', 'mises', 'bayed', 'ahmet', 'sings', 'horde', 'aides', 'aided', 'howry']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_words_types = set(brown_words_tokens)\n",
    "print(len(brown_words_types))\n",
    "print((list(brown_words_types)[:10]))\n",
    "\n",
    "missing_words_brown = set()\n",
    "for word in official_words:\n",
    "    if word not in brown_words_types:\n",
    "        missing_words_brown.add(word)\n",
    "        \n",
    "len(missing_words_brown)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8586\n"
     ]
    }
   ],
   "source": [
    "treebank_words_tokens = []\n",
    "\n",
    "for word in treebank.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            treebank_words_tokens.append(word)\n",
    "\n",
    "print(len(treebank_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096\n",
      "['gangs', 'bronx', 'sites', 'cosby', 'binge', 'aides', 'jumps', 'heard', 'lizhi', 'foods']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1706"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_words_types = set(treebank_words_tokens)\n",
    "print(len(treebank_words_types))\n",
    "print((list(treebank_words_types)[:10]))\n",
    "\n",
    "missing_words_treebank = set()\n",
    "for word in official_words:\n",
    "    if word not in treebank_words_types:\n",
    "        missing_words_treebank.add(word)\n",
    "        \n",
    "len(missing_words_treebank)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### switchboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6393\n"
     ]
    }
   ],
   "source": [
    "switchboard_words_tokens = []\n",
    "\n",
    "for word in switchboard.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            switchboard_words_tokens.append(word)\n",
    "\n",
    "print(len(switchboard_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656\n",
      "['gangs', 'choke', 'cosby', 'greek', 'heard', 'worry', 'rexes', 'coils', 'first', 'songs']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1864"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchboard_words_types = set(switchboard_words_tokens)\n",
    "print(len(switchboard_words_types))\n",
    "print((list(switchboard_words_types)[:10]))\n",
    "\n",
    "missing_words_switchboard = set()\n",
    "for word in official_words:\n",
    "    if word not in switchboard_words_types:\n",
    "        missing_words_switchboard.add(word)\n",
    "        \n",
    "len(missing_words_switchboard)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249307\n"
     ]
    }
   ],
   "source": [
    "gutenberg_words_tokens = []\n",
    "\n",
    "for word in gutenberg.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            gutenberg_words_tokens.append(word)\n",
    "\n",
    "print(len(gutenberg_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4684\n",
      "['keziz', 'anted', 'wince', 'becam', 'bayed', 'mower', 'sings', 'thara', 'horde', 'emims']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg_words_types = set(gutenberg_words_tokens)\n",
    "print(len(gutenberg_words_types))\n",
    "print((list(gutenberg_words_types)[:10]))\n",
    "\n",
    "missing_words_gutenberg = set()\n",
    "for word in official_words:\n",
    "    if word not in gutenberg_words_types:\n",
    "        missing_words_gutenberg.add(word)\n",
    "        \n",
    "len(missing_words_gutenberg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163588\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_words_tokens = []\n",
    "\n",
    "for word in movie_reviews.words():\n",
    "    word = word.lower()\n",
    "    if len(word) == 5:\n",
    "        if word.isalpha():\n",
    "            movie_reviews_words_tokens.append(word)\n",
    "\n",
    "print(len(movie_reviews_words_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4508\n",
      "['wince', 'thyme', 'motss', 'ravel', 'hauls', 'ahmet', 'mower', 'sings', 'nears', 'horde']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews_words_types = set(movie_reviews_words_tokens)\n",
    "print(len(movie_reviews_words_types))\n",
    "print((list(movie_reviews_words_types)[:10]))\n",
    "\n",
    "missing_words_movie_reviews = set()\n",
    "for word in official_words:\n",
    "    if word not in movie_reviews_words_types:\n",
    "        missing_words_movie_reviews.add(word)\n",
    "        \n",
    "len(missing_words_movie_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grand Corpus - 5 letters\n",
    "- 8043 types\n",
    "- only 177 missing from official wordle list (2132 / 2109, 92% mutual)\n",
    "- written to text file and put in working directory (put this in another .ipynb called \"data processing\" or something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535188\n",
      "['years', 'board', 'dutch', 'group', 'agnew', 'years', 'named', 'among', 'group', 'years', 'fiber', 'lungs', 'brief', 'later', 'loews', 'makes', 'using', 'today', 'forum', 'bring']\n"
     ]
    }
   ],
   "source": [
    "all_corpora = [treebank_words_tokens, brown_words_tokens, gutenberg_words_tokens, switchboard_words_tokens, movie_reviews_words_tokens]\n",
    "grand_corpus_tokens = []\n",
    "for corpus in all_corpora:\n",
    "    for word in corpus:\n",
    "        grand_corpus_tokens.append(word)\n",
    "\n",
    "print(len(grand_corpus_tokens))\n",
    "print(grand_corpus_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### writing just words to .txt file\n",
    "with open(\"data/nltk_grand_corpus_tokens_5.txt\", \"w\") as fout:\n",
    "    for word in grand_corpus_tokens:\n",
    "        fout.write(word + \"\\n\")\n",
    "\n",
    "grand_corpus_word_freq = get_word_distribution(grand_corpus_tokens, sort = \"descending\")\n",
    "\n",
    "### writing words and counts to .txt file\n",
    "with open(\"data/nltk_grand_corpus_types_and_counts_5.txt\", \"w\") as fout:\n",
    "    for word, count in grand_corpus_word_freq:\n",
    "        fout.write(word + \"\\t\" + str(count) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8043\n",
      "['keziz', 'thyme', 'hauls', 'ahmet', 'thara', 'nears', 'sakes', 'weepy', 'maron', 'libby', 'flush', 'sayle', 'skunk', 'fucks', 'latit', 'digge', 'cocoa', 'balak', 'ornan', 'stacy']\n",
      "177\n",
      "['zonal', 'fizzy', 'wooer', 'torus', 'girly', 'tuber', 'krill', 'toddy', 'creme', 'slosh', 'pleat', 'vegan', 'duchy', 'rayon', 'decal', 'aping', 'frond', 'bleep', 'rearm', 'enema']\n"
     ]
    }
   ],
   "source": [
    "grand_corpus_types = set(grand_corpus_tokens)\n",
    "print(len(grand_corpus_types))\n",
    "print(list(grand_corpus_types)[:20])\n",
    "\n",
    "grand_corpus_missing = []\n",
    "for word in official_words:\n",
    "    if word not in grand_corpus_types:\n",
    "        grand_corpus_missing.append(word)\n",
    "\n",
    "print(len(grand_corpus_missing))\n",
    "print(list(grand_corpus_missing)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most frequent words in grand corpus:\n",
      "\n",
      "[('which', 15760), ('their', 13925), ('there', 13447), ('shall', 11997), ('would', 9361), ('about', 8388), ('could', 6851), ('other', 6340), ('these', 5859), ('movie', 5826), ('after', 5686), ('first', 5294), ('great', 4738), ('where', 4403), ('every', 4398), ('never', 4030), ('house', 3960), ('being', 3789), ('those', 3657), ('while', 3534)]\n",
      "\n",
      "20 least frequent words in grand corpus:\n",
      "\n",
      "[('agnew', 1), ('borge', 1), ('menem', 1), ('imsai', 1), ('gingl', 1), ('harpo', 1), ('chary', 1), ('kuala', 1), ('shrum', 1), ('kelli', 1), ('nelms', 1), ('desai', 1), ('kuhns', 1), ('erode', 1), ('kuvin', 1), ('soups', 1), ('coors', 1), ('spiro', 1), ('milne', 1), ('rotie', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_freq_20 = get_word_distribution(grand_corpus_tokens, sort = \"descending\")[:20]\n",
    "least_freq_20 = get_word_distribution(grand_corpus_tokens, sort = \"ascending\")[:20]\n",
    "print(f\"20 most frequent words in grand corpus:\\n\\n{most_freq_20}\\n\")\n",
    "print(f\"20 least frequent words in grand corpus:\\n\\n{least_freq_20}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('which', 15760),\n",
       " ('their', 13925),\n",
       " ('there', 13447),\n",
       " ('shall', 11997),\n",
       " ('would', 9361),\n",
       " ('about', 8388),\n",
       " ('could', 6851),\n",
       " ('other', 6340),\n",
       " ('these', 5859),\n",
       " ('movie', 5826)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grand_freqs = get_word_distribution(grand_corpus_tokens, sort = \"descending\")\n",
    "grand_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132\n"
     ]
    }
   ],
   "source": [
    "wordle_freq_ratings = []\n",
    "for word in official_words:\n",
    "    for tup in grand_freqs:\n",
    "        if tup[0] == word:\n",
    "            wordle_freq_ratings.append(tup)\n",
    "\n",
    "found_words_sorted = sorted(wordle_freq_ratings, key = operator.itemgetter(1), reverse = True) # sorted descending\n",
    "print(len(found_words_sorted)) # 2132 of 2309 wordle words will have a frequency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grand Corpus - Other Word Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Takes 50s to run cell\n",
    "\n",
    "grand_corpus_tokens_3 = []\n",
    "grand_corpus_tokens_4 = []\n",
    "# grand_corpus_tokens_5 = []\n",
    "grand_corpus_tokens_6 = []\n",
    "grand_corpus_tokens_7 = []\n",
    "grand_corpus_tokens_8 = []\n",
    "grand_corpus_tokens_9 = []\n",
    "grand_corpus_tokens_10 = []\n",
    "\n",
    "tokens_lists = [\n",
    "    (grand_corpus_tokens_3, 3), (grand_corpus_tokens_4, 4), \n",
    "    # (grand_corpus_tokens_5, 5), \n",
    "    (grand_corpus_tokens_6, 6),\n",
    "    (grand_corpus_tokens_7, 7), (grand_corpus_tokens_8, 8),\n",
    "    (grand_corpus_tokens_9, 9), (grand_corpus_tokens_10, 10)]\n",
    "\n",
    "corpora = [brown, treebank, switchboard, gutenberg, movie_reviews]\n",
    "\n",
    "for corpus in corpora:\n",
    "\n",
    "    for tokens_list, word_len in tokens_lists:\n",
    "\n",
    "        for word in corpus.words():\n",
    "            word = word.lower()\n",
    "            if len(word) == word_len:\n",
    "                if word.isalpha():\n",
    "                    tokens_list.append(word)\n",
    "\n",
    "grand_corpus_types_3 = set(grand_corpus_tokens_3)\n",
    "grand_corpus_types_4 = set(grand_corpus_tokens_4)\n",
    "# grand_corpus_types_5 = set(grand_corpus_tokens_5)\n",
    "grand_corpus_types_6 = set(grand_corpus_tokens_6)\n",
    "grand_corpus_types_7 = set(grand_corpus_tokens_7)\n",
    "grand_corpus_types_8 = set(grand_corpus_tokens_8)\n",
    "grand_corpus_types_9 = set(grand_corpus_tokens_9)\n",
    "grand_corpus_types_10 = set(grand_corpus_tokens_10)\n",
    "\n",
    "print(len(grand_corpus_types_3))\n",
    "print((list(grand_corpus_types_3)[:7]))\n",
    "print(len(grand_corpus_types_4))\n",
    "print((list(grand_corpus_types_4)[:7]))\n",
    "# print(len(grand_corpus_types_5))\n",
    "# print((list(grand_corpus_types_5)[:7]))\n",
    "print(len(grand_corpus_types_6))\n",
    "print((list(grand_corpus_types_6)[:7]))\n",
    "print(len(grand_corpus_types_7))\n",
    "print((list(grand_corpus_types_7)[:7]))\n",
    "print(len(grand_corpus_types_8))\n",
    "print((list(grand_corpus_types_8)[:7]))\n",
    "print(len(grand_corpus_types_9))\n",
    "print((list(grand_corpus_types_9)[:7]))\n",
    "print(len(grand_corpus_types_10))\n",
    "print((list(grand_corpus_types_10)[:7]))\n",
    "\n",
    "tokens_lists = [\n",
    "    (grand_corpus_tokens_3, 3), (grand_corpus_tokens_4, 4), \n",
    "    # (grand_corpus_tokens_5, 5), \n",
    "    (grand_corpus_tokens_6, 6),\n",
    "    (grand_corpus_tokens_7, 7), (grand_corpus_tokens_8, 8),\n",
    "    (grand_corpus_tokens_9, 9), (grand_corpus_tokens_10, 10)]\n",
    "\n",
    "for token_list, word_len in tokens_lists:\n",
    "\n",
    "    tokens_list_word_freq = get_word_distribution(token_list, sort = \"descending\")\n",
    "\n",
    "    ### writing types and counts to .txt file\n",
    "    with open(f\"data/nltk_grand_corpus_types_and_counts_{word_len}.txt\", \"w\") as fout:\n",
    "        for word, count in tokens_list_word_freq:\n",
    "            fout.write(word + \"\\t\" + str(count) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
