{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "import time\n",
    "import pandas as pd\n",
    "import Levenshtein\n",
    "from wordle_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### official words\n",
    "- official wordle word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wince', 'thyme', 'mower', 'horde', 'heard']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Official list\n",
    "official_words = []\n",
    "\n",
    "with open(\"data/official_words_processed.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    for word in f.read().split(\"\\n\"):\n",
    "        if word.isalpha():\n",
    "            official_words.append(word)\n",
    "\n",
    "f.close() # closes connection to file\n",
    "\n",
    "print(len(official_words))\n",
    "official_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache\n",
      "yacht\n",
      "macho\n"
     ]
    }
   ],
   "source": [
    "for word in official_words:\n",
    "    if word[1:4] == \"ach\":\n",
    "        print(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix/Suffix bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes:\n",
      "214\n",
      "[('st', 65), ('sh', 52), ('cr', 45), ('sp', 45), ('ch', 40), ('gr', 38), ('fl', 36), ('re', 36), ('tr', 36), ('br', 35)]\n",
      "-----\n",
      "Suffixes:\n",
      "202\n",
      "[('er', 141), ('ly', 56), ('ch', 56), ('se', 52), ('al', 49), ('ck', 47), ('ty', 46), ('te', 39), ('el', 38), ('dy', 38)]\n"
     ]
    }
   ],
   "source": [
    "suffix_freq_dist = {}\n",
    "prefix_freq_dist = {}\n",
    "\n",
    "for word in official_words:\n",
    "    prefix = word[:2] # first 2 letters\n",
    "    suffix = word[-2:] # last 2 letters\n",
    "    if prefix not in prefix_freq_dist:\n",
    "        prefix_freq_dist[prefix] = 1\n",
    "    else:\n",
    "        prefix_freq_dist[prefix] += 1\n",
    "\n",
    "    if suffix not in suffix_freq_dist:\n",
    "        suffix_freq_dist[suffix] = 1\n",
    "    else:\n",
    "        suffix_freq_dist[suffix] += 1\n",
    "\n",
    "suffix_types = [key for key in suffix_freq_dist.keys()]\n",
    "prefix_types = [key for key in prefix_freq_dist.keys()]\n",
    "\n",
    "sorted_prefix_dist = sorted(prefix_freq_dist.items(), key = operator.itemgetter(1), reverse = True)\n",
    "sorted_suffix_dist = sorted(suffix_freq_dist.items(), key = operator.itemgetter(1), reverse = True)\n",
    "\n",
    "print(\"Prefixes:\")\n",
    "print(len(sorted_prefix_dist))\n",
    "print(sorted_prefix_dist[:10])\n",
    "print(\"-----\")\n",
    "print(\"Suffixes:\")\n",
    "print(len(sorted_suffix_dist))\n",
    "print(sorted_suffix_dist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ing', 31),\n",
       " ('lly', 22),\n",
       " ('ove', 21),\n",
       " ('ver', 21),\n",
       " ('sta', 21),\n",
       " ('ast', 20),\n",
       " ('lea', 19),\n",
       " ('ter', 19),\n",
       " ('tch', 19),\n",
       " ('sha', 18),\n",
       " ('ine', 18),\n",
       " ('ate', 18),\n",
       " ('sto', 18),\n",
       " ('ide', 18),\n",
       " ('out', 18)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams_freq_dist = {}\n",
    "gram_len = 3\n",
    "\n",
    "for word in official_words:\n",
    "    for i in range(0, len(word) - (gram_len - 1)): # so it doesn't index out of range\n",
    "        gram = word[i:i + gram_len]\n",
    "\n",
    "        if gram not in grams_freq_dist:\n",
    "            grams_freq_dist[gram] = 1\n",
    "        else:\n",
    "            grams_freq_dist[gram] += 1\n",
    "\n",
    "print(len(grams_freq_dist))\n",
    "sorted_gram_dist = sorted(grams_freq_dist.items(), key = operator.itemgetter(1), reverse = True)\n",
    "sorted_gram_dist[:15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_guesses(word_list: list, perf_letters: list, incorr_pos: list, wrong_letters: list):\n",
    "    \"\"\"\n",
    "    This function takes a single Wordle guess and feedback returned by the game about that guess, and returns a list of possible remaining guesses.\n",
    "\n",
    "    Parameters:\n",
    "    -----\n",
    "    `word_list`: list\n",
    "        list of all possible words immediately prior to this stage's guess\n",
    "    `perf_letters_letters`: list\n",
    "        list of tuples, where the structure of each tuple is: (\"correct_letter\", int of letter position). Example listed below\n",
    "    `incorr_pos`: list\n",
    "        list of tuples, same structure as `perf_letters_letters`, but with letters and their incorrect positions\n",
    "    `wrong_letters`: list\n",
    "        list of individual letters that are not in the target word whatsoever\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    `potentials_list`: list\n",
    "        list of words that remain after eliminating all words based on the information provided\n",
    "\n",
    "    Examples of inputs:\n",
    "    -----\n",
    "    word_list = official_words # something like ['wince', 'thyme', 'mower', 'horde', 'heard', 'tenor', 'zonal', 'parry', 'shied', 'fizzy']\n",
    "    \n",
    "    perf_letters = [(\"r\", 2)] # could be any number of tuples\n",
    "    \n",
    "    incorr_pos = [(\"t\", 2), (\"r\", 4)] # could be any number of tuples\n",
    "    \n",
    "    wrong_letters_letters = [\"l\", \"a\", \"e\"] # could be any number of items\n",
    "    \"\"\"\n",
    "    \n",
    "    incorr_words = set() # set of all words that the target could not possibly be the target\n",
    "    for word in word_list:\n",
    "        if len(incorr_pos) > 0: # sometimes there are none\n",
    "            for incorr_letter, pos in incorr_pos: # adding words that have words of incorr letter positions (but not words that have these letters altogether - they could just be in a different spot)\n",
    "                if 0 <= pos <= 4:\n",
    "                    # print (word, pos)\n",
    "                    if word[pos] == incorr_letter:\n",
    "                        # print (word[pos])\n",
    "                        incorr_words.add(word)\n",
    "        if len(wrong_letters) > 0: # sometimes there are none\n",
    "            for bad_letter in wrong_letters: # adding words that have completely wrong letters in them\n",
    "                if bad_letter in word:\n",
    "                    incorr_words.add(word)\n",
    "\n",
    "    intermediate_list = set(word_list).difference(incorr_words) # difference between all impossible words and entire original passed word_list\n",
    "    # print(intermediate_list)\n",
    "    potentials_list = set()\n",
    "\n",
    "    for word in intermediate_list:\n",
    "        if len(perf_letters) > 0: # sometimes there are none\n",
    "            good_letters = []\n",
    "            for letter, pos in perf_letters:\n",
    "                if 0 <= pos <= 4:\n",
    "                    if word[pos] == letter:\n",
    "                        good_letters.append(letter)\n",
    "                if len(good_letters) == len(perf_letters):\n",
    "                    potentials_list.add(word)\n",
    "                else:\n",
    "                    pass # skip to the next word\n",
    "    \n",
    "    if len(potentials_list) > 0:\n",
    "        potentials_list = list(potentials_list)\n",
    "    else:\n",
    "        potentials_list = list(intermediate_list)\n",
    "\n",
    "    return list(potentials_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('until', 27.58),\n",
       " ('unlit', 27.58),\n",
       " ('glint', 26.12),\n",
       " ('tunic', 25.73),\n",
       " ('tulip', 25.69),\n",
       " ('unity', 25.45),\n",
       " ('guilt', 25.26),\n",
       " ('flint', 25.25),\n",
       " ('built', 24.96),\n",
       " ('input', 24.79)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_pos = [(\"p\", 3)]\n",
    "incor_pos = [('t', 1), ('a', 2), ('e', 4)]\n",
    "wrong_letts = ['t']\n",
    "\n",
    "pos_list = get_possible_guesses(word_list = official_words, perf_letters = corr_pos, incorr_pos = incor_pos, wrong_letters = wrong_letts)\n",
    "\n",
    "test_ratings = get_word_meaning(word_list = official_words, words_to_rate = official_words, normalized = False, unique = True)\n",
    "test_guess = \"arose\"\n",
    "\n",
    "opposite_guesses = set()\n",
    "for word, rating in test_ratings:\n",
    "    if len(set(word).difference(set(test_guess))) == 5:\n",
    "        if word != test_guess: # to make sure it doesn't loop infinitely\n",
    "            opposite_guesses.add(word)\n",
    "\n",
    "opposite_ratings = get_word_meaning(word_list = official_words, words_to_rate = list(opposite_guesses), normalized = False, unique = True)\n",
    "opposite_ratings[:10]\n",
    "\n",
    "#### NEXT\n",
    "## if len(opposite_ratings) == 0, then use the highest rated word with common letters (or could try going to next highest rated word)\n",
    "# (or could try going to next highest rated word that has 4 in common, instead of 5. If len(that list) == 0, go to 3, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_dists(word_list: list, rating_method: str = \"normalized\"):\n",
    "    \"\"\"\n",
    "    Given a list of words of the same length, creates a list of lists of tuples of (letter, count) in that position of the word. Returned list's indexing corresponds to letter positions of length of words within passed word list.\n",
    "\n",
    "    Parameters:\n",
    "    -----\n",
    "    `word_list`: list\n",
    "        list of words (str) of the same length\n",
    "    `rating_method`: bool\n",
    "        if \"normalized\", new ratings are normalized in the same way previous ratings were. If \"ranked\", new ratings are just a percentile score relative to their ranking within the list after multiplier is applied.\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    `dists_list`: list\n",
    "        list of lists of sorted letters distributions (tuples)\n",
    "    \"\"\"\n",
    "\n",
    "    dists_list = []\n",
    "\n",
    "    for i in range(0, len(word_list[0])):\n",
    "\n",
    "        test_dict = {}\n",
    "\n",
    "        for word in word_list:\n",
    "            # if word.islpha():\n",
    "            if word.isalpha():\n",
    "                if word[i] in test_dict:\n",
    "                    test_dict[word[i]] += 1\n",
    "                else:\n",
    "                    test_dict[word[i]] = 1\n",
    "\n",
    "        sorted_dist = sorted(test_dict.items(), key = operator.itemgetter(1), reverse = True) # sorted descending\n",
    "\n",
    "        # ratings calculated as normalized new ratings\n",
    "        if rating_method == \"normalized\":\n",
    "            if len(sorted_dist) > 1:\n",
    "                sort_dist_normd = []\n",
    "                for tup in sorted_dist:\n",
    "                    # try:\n",
    "                    normd = (tup[1] - sorted_dist[-1][1]) / (sorted_dist[0][1] - sorted_dist[-1][1])\n",
    "                    sort_dist_normd.append((tup[0], normd))\n",
    "                    # except:\n",
    "                    #     ZeroDivisionError\n",
    "                    #     # print(sorted_dist[-1][1])\n",
    "                    #     print(\"TESTESTSETST\")\n",
    "                    #     sort_dist_normd.append((tup[0], 0.0))   \n",
    "            # print (sort_dist_normd[-2][1])\n",
    "            # if sort_dist_normd[-1][1] == 0:\n",
    "            #     sort_dist_normd[-1][1] = sort_dist_normd[-2][1] / 2\n",
    "            #     new_tup = sort_dist_normd[-1][1]\n",
    "                \n",
    "                    \n",
    "        # ratings calculated according to ranking percentile\n",
    "        if rating_method == \"ranked\":\n",
    "            sort_dist_normd = []\n",
    "            for tup in sorted_dist:\n",
    "                letter = tup[0]\n",
    "                rating = float((len(sorted_dist) - sorted_dist.index(tup)) / (len(sorted_dist)))\n",
    "                sort_dist_normd.append((letter, rating))\n",
    "            \n",
    "            sort_dist_normd = sorted(sort_dist_normd, key = operator.itemgetter(1), reverse = True) # sorted descending\n",
    "        \n",
    "        dists_list.append(sort_dist_normd)\n",
    "\n",
    "    return dists_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('t', 1.0), ('p', 0.5)],\n",
       " [('e', 1.0), ('a', 0.5)],\n",
       " [('s', 1.0), ('r', 0.5)],\n",
       " [('t', 1.0)],\n",
       " [('s', 1.0), ('y', 0.5)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_positional_dists(['tests', 'party'], rating_method = \"ranked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intensify_by_positions(word_list: list, ratings_list: list, strength: float = 1, rounding: int = None):\n",
    "    \"\"\"\n",
    "    Given a passed list of tuples containing words of equal length and a rating for each word, this function multiplies each word's passed rating by an evaluation of how likely each letter in the word is to appear in its current position.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    `ratings_list`: list\n",
    "        list of tuples. Structure is ('letter', rating)\n",
    "    `strength`: float\n",
    "        how strongly the intensifier should affect the current ratings. Default is 1 (\"normal\" strength)\n",
    "    `rounding`: int\n",
    "        Number of decimal places to round to\n",
    "    Returns:\n",
    "    ------\n",
    "    `sorted_new_ratings`: list\n",
    "        list of tuples of the same words that were passed, but ratings have been multiplied according to the average likelihood for the each letter in the word to appear in its position.\n",
    "    \"\"\"\n",
    "\n",
    "    pos_dists = create_positional_dists(word_list = word_list, rating_method = \"normalized\")\n",
    "\n",
    "    new_ratings = []\n",
    "\n",
    "    for tup in ratings_list:\n",
    "        word = tup[0]\n",
    "        curr_rating = tup[1]\n",
    "        \n",
    "        intensifiers_sum = 0\n",
    "        \n",
    "        for char_id in range(0, len(word)): # iterate through chars in word\n",
    "\n",
    "            char = word[char_id]\n",
    "            # for ex_tup in example[char_id]: # iterate through the list that corresponds to the character position we're currently in of the word \n",
    "            for ex_tup in pos_dists[char_id]: # iterate through the list that corresponds to the character position we're currently in of the word \n",
    "                if ex_tup[0] == char:\n",
    "                    intensifiers_sum += ex_tup[1] # add the intensifier for that letter from that list to the sum\n",
    "                    # print(char, ex_tup[1])\n",
    "                    break\n",
    "\n",
    "        intensifier = float(intensifiers_sum / len(word))\n",
    "        # print(word, intensifier)\n",
    "        # final_rating = intensifier / curr_rating * strength\n",
    "        final_rating = (1 - intensifier) / curr_rating * strength\n",
    "\n",
    "        if rounding:\n",
    "            final_rating = round(final_rating, rounding)\n",
    "        # else:\n",
    "        #     final_rating = final_rating\n",
    "\n",
    "        new_ratings.append((word, final_rating))\n",
    "        sorted_new_ratings = sorted(new_ratings, key = operator.itemgetter(1), reverse = True) # sorted descending\n",
    "\n",
    "    return sorted_new_ratings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('later', 0.022424851940517614),\n",
       " ('party', 0.014627427808230845),\n",
       " ('tests', 0.013616171978570898)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = [(\"tests\", 89), (\"party\", 49), (\"later\", 34)]\n",
    "\n",
    "intensify_by_positions(word_list = official_words, ratings_list = tests, strength = 2, rounding = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordle_wizard() 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grams(word_list: list, length: int = 2, from_start: bool = False, perc: bool = True, rounding: int = None):\n",
    "    \"\"\"\n",
    "    Get a distribution of all grams of indicated length from passed list of words. Words must be each same length or else indexing issues may occur.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    `word_list`: list\n",
    "        list of words of the same length\n",
    "    `length`: int\n",
    "        length of desired gram\n",
    "    `from_start`: bool\n",
    "        if True, gram length is counted from the start of each word. If False, length is counted from the end\n",
    "    `perc`: bool\n",
    "        if True, normalizes returned values as a percentage of sum of all counts. If False, returns respective counts\n",
    "    `rounding`: int\n",
    "        if not None, rounds returned values to nearest int decimal place\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    `gram_dist`: list\n",
    "        list of tuples in structure [(gram, count), (gram, count), ...] in descending order of counts\n",
    "    \"\"\"\n",
    "\n",
    "    gram_dist = {}\n",
    "    for word in word_list:\n",
    "        if from_start == True: # if prefix\n",
    "            gram = word[:length]\n",
    "        else: # if suffix\n",
    "            gram = word[-length:]\n",
    "        if gram not in gram_dist:\n",
    "            gram_dist[gram] = 1\n",
    "        else:\n",
    "            gram_dist[gram] += 1\n",
    "\n",
    "    gram_dist = sorted(gram_dist.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    gram_dist[:10]\n",
    "    perc_dist = []\n",
    "\n",
    "    if perc == True:    \n",
    "        for gram, count in gram_dist:\n",
    "            if rounding: # round result to indicated decimal place\n",
    "                norm_score = round(((count / sum([c for g, c in gram_dist])) * 100), rounding)\n",
    "            else: # don't round at all\n",
    "                norm_score = (count / sum([c for g, c in gram_dist])) * 100\n",
    "            perc_dist.append((gram, norm_score))\n",
    "\n",
    "        gram_dist = sorted(perc_dist, key = operator.itemgetter(1), reverse = True)\n",
    "\n",
    "    return gram_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('er', 141), ('ly', 56), ('ch', 56), ('se', 52), ('al', 49)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_grams(word_list = official_words, length = 2, from_start = False, perc = False, rounding = 4)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordle_wizard2(guess: str, target: str, word_list: list, opt_thres: int = 100, max_guesses: int = 6, gram_bias: str = None, intensifier: bool = False, mult_strength: float = 1, return_stats: bool = False, drama: float = 0, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Wordle Wizard 2.0. New and improved.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    `guess`: str\n",
    "        starting word\n",
    "    `target`: str\n",
    "        word for the model to find a path to\n",
    "    `word_list`: list\n",
    "        list of possible words. All words must be of same length\n",
    "    `opt_thres`: int > 1\n",
    "        after each guess is evaluated and all remaining possible words are found, opt_thres is the \"optimizer\" threshold above which the model will choose a new maximally diverse, highest rated word with all new letters, instead of using known correct letters\n",
    "    `max_guesses`: int\n",
    "        max number of guesses allowed before puzzle is considered unsolved. Default value is 6, matching official game\n",
    "    `gram_bias`: str\n",
    "        if \"suffix\", biases guessing towards words with more common suffixes. If \"prefix\", biases guessing towards words with more common prefixes\n",
    "    `intensifier`: bool\n",
    "        if True, ratings of each word are not just calculated based on letter diversity of each word, but also positionality of letters in each word\n",
    "    `mult_strength`: float\n",
    "        float value that affects the strength of the intensifier of the word ratings of all possible remaining words generated after each guess\n",
    "    `return_stats`: bool\n",
    "        if True, returns a dictionary of statistics and information about playthrough\n",
    "    `verbose`: bool\n",
    "        if True, print extra information as solution as found. If False, prints only guesses and found target word\n",
    "    `drama`: float\n",
    "        amount of time to wait at certain points as the functions runs. For dramatic effect and literally nothing else\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    `stats_dict`: dict\n",
    "        dictionary of metrics tracked about a wordle playthrough to make playthroughs comparable\n",
    "    \"\"\"\n",
    "\n",
    "    guess = guess.lower() # lowering for consistency\n",
    "    target = target.lower() # lowering for consistency\n",
    "   \n",
    "    if guess not in word_list and len(guess) == 5: word_list.append(guess)\n",
    "    if target not in word_list and len(target) == 5: word_list.append(target)\n",
    "\n",
    "    stats_dict = {}\n",
    "    stats_dict['first_guess'] = guess\n",
    "    stats_dict['target'] = target\n",
    "    stats_dict['intensifier'] = intensifier\n",
    "    stats_dict['mult_strength'] = mult_strength\n",
    "    stats_dict['opt_thresh'] = opt_thres\n",
    "    stats_dict['first_guess_vows'] = count_vows_cons(guess, y_vow = True)['vows']\n",
    "    stats_dict['first_guess_cons'] = count_vows_cons(guess, y_vow = True)['cons']\n",
    "    stats_dict['target_vows'] = count_vows_cons(target, y_vow = True)['vows']\n",
    "    stats_dict['target_cons'] = count_vows_cons(target, y_vow = True)['cons']\n",
    "    \n",
    "    avg_guess_meaning = []\n",
    "    all_guessed_letts = set() # running total of all unique guessed letters (corrent and incorrect) -- used for optimal mode\n",
    "\n",
    "    wordlen = len(guess)\n",
    "    ideal_letter_diversity = len(guess) # making a second one because this variable is decremented and I dont' want wordlen to be affected\n",
    "    \n",
    "    letter_positions = set(i for i in range(0, wordlen))\n",
    "    \n",
    "    guess_num = 0\n",
    "\n",
    "    guessed_words = []\n",
    "    perf_letters = []\n",
    "    incorr_pos = []\n",
    "    wrong_letters = []\n",
    "    vows_per_guess = []\n",
    "    cons_per_guess = []\n",
    "    words_remaining = []\n",
    "    if gram_bias == \"suffix\":\n",
    "        word_list_grams = get_grams(word_list = official_words, length = 2, from_start = False, perc = True, rounding = 4)[:5]\n",
    "    elif gram_bias == \"prefix\":\n",
    "        word_list_grams = get_grams(word_list = official_words, length = 2, from_start = True, perc = True, rounding = 4)[:5]\n",
    "\n",
    "    while guess:\n",
    "\n",
    "        guess_num += 1\n",
    "\n",
    "        if return_stats == False:\n",
    "            time.sleep(drama)\n",
    "            if guess_num == 1:\n",
    "                print(\"------------------\")\n",
    "            # if verbose ==\n",
    "            \n",
    "            print(f\"\\nGuess {guess_num}: '{guess}'\\n\")\n",
    "            if verbose == False:\n",
    "                print(\"------------------\")\n",
    "        \n",
    "        # stats tracked about each guess\n",
    "        guessed_words.append(guess)\n",
    "        all_guessed_letts.update(set(guess)) # set of all used letters -- used in \"optimal\" mode\n",
    "        vows_per_guess.append(count_vows_cons(guess, y_vow = True)['vows'])\n",
    "        cons_per_guess.append(count_vows_cons(guess, y_vow = True)['cons'])\n",
    "\n",
    "        ### once target has been guessed, do all of these\n",
    "        if guess == target:\n",
    "            if return_stats == False:\n",
    "                if verbose == False:\n",
    "                    print(f\"\\nThe puzzle was solved after {guess_num} guesses.\")\n",
    "                else:\n",
    "                    print(f\"The puzzle was solved after {guess_num} guesses.\")\n",
    "                print(f\"The target word was '{target}'.\\n\")\n",
    "                print(\"------------------\")\n",
    "            if return_stats == True:\n",
    "                stats_dict['num_guesses'] = guess_num\n",
    "                stats_dict['avg_guess_meaning'] =  0 if sum(avg_guess_meaning) == 0 else round(sum(avg_guess_meaning) / len(avg_guess_meaning), 2)\n",
    "                stats_dict['avg_vows_per_guess'] = sum(vows_per_guess) / len(vows_per_guess)\n",
    "                stats_dict['avg_cons_per_guess'] = sum(cons_per_guess) / len(cons_per_guess)\n",
    "                stats_dict['valid_solution'] = True if guess_num <= 6 else False\n",
    "                stats_dict['gram_bias'] = str(gram_bias)\n",
    "\n",
    "                if len(guessed_words) > 1:\n",
    "                    lev_dists = []\n",
    "                    for i in range(1, len(guessed_words)):\n",
    "                        dist = Levenshtein.jaro_winkler(guessed_words[i], guessed_words[i-1])\n",
    "                        lev_dists.append(dist)\n",
    "                    stats_dict['avg_similarity'] = sum(lev_dists) / len(lev_dists)\n",
    "                    stats_dict['luck'] = (sum(lev_dists) / len(lev_dists)) / guess_num\n",
    "                    stats_dict['avg_words_remaining'] = sum(words_remaining) / len(words_remaining)\n",
    "                else: # if first guess is target\n",
    "                    stats_dict['avg_similarity'] = 1\n",
    "                    stats_dict['luck'] = 1\n",
    "                    stats_dict['avg_words_remaining'] = 0\n",
    "\n",
    "                return stats_dict\n",
    "            break\n",
    "        \n",
    "        ### if max_guesses is reached\n",
    "        if guess_num == max_guesses:            \n",
    "            if return_stats == False:\n",
    "                if verbose == False:\n",
    "                    print(\"\\n\")\n",
    "                print(f\"The puzzle could not be solved within the maximum number of guesses.\")\n",
    "                print(f\"The target word was '{target}'.\\n\")\n",
    "                print(\"------------------\")            \n",
    "            else:\n",
    "                stats_dict['num_guesses'] = guess_num\n",
    "                stats_dict['avg_guess_meaning'] =  0 if sum(avg_guess_meaning) == 0 else round(sum(avg_guess_meaning) / len(avg_guess_meaning), 2)\n",
    "                stats_dict['avg_vows_per_guess'] = sum(vows_per_guess) / len(vows_per_guess)\n",
    "                stats_dict['avg_cons_per_guess'] = sum(cons_per_guess) / len(cons_per_guess)\n",
    "                stats_dict['valid_solution'] = True if guess_num <= 6 else False\n",
    "                stats_dict['gram_bias'] = str(gram_bias)\n",
    "\n",
    "                if len(guessed_words) > 1:\n",
    "                    lev_dists = []\n",
    "                    for i in range(1, len(guessed_words)):\n",
    "                        dist = Levenshtein.jaro_winkler(guessed_words[i], guessed_words[i-1])\n",
    "                        lev_dists.append(dist)\n",
    "                    stats_dict['avg_similarity'] = sum(lev_dists) / len(lev_dists)\n",
    "                    stats_dict['luck'] = (sum(lev_dists) / len(lev_dists)) / guess_num\n",
    "                    stats_dict['avg_words_remaining'] = sum(words_remaining) / len(words_remaining)\n",
    "                else: # if first guess is target\n",
    "                    stats_dict['avg_similarity'] = 1\n",
    "                    stats_dict['luck'] = 1\n",
    "                    stats_dict['avg_words_remaining'] = 0\n",
    "                \n",
    "                return stats_dict\n",
    "            break\n",
    "        \n",
    "        #### Evaluating current guess according to 3 checks of the game\n",
    "        for i in letter_positions: # number of letters in each word (current word and target word)\n",
    "\n",
    "            if guess[i] == target[i]:\n",
    "                if (guess[i], i) not in perf_letters:\n",
    "                    perf_letters.append((guess[i], i))\n",
    "            elif guess[i] != target[i] and guess[i] in target:\n",
    "                if (guess[i], i) not in incorr_pos:\n",
    "                    incorr_pos.append((guess[i], i))\n",
    "            elif guess[i] not in target:\n",
    "                if (guess[i]) not in wrong_letters:\n",
    "                    # wrong_letters.add(guess[i])\n",
    "                    wrong_letters.append(guess[i])\n",
    "                    wrong_letters = list(set(wrong_letters))\n",
    "\n",
    "        perf_letters = sorted(perf_letters, key = operator.itemgetter(1), reverse = False)\n",
    "        incorr_pos = sorted(incorr_pos, key = operator.itemgetter(1), reverse = False)\n",
    "        wrong_letters = sorted(wrong_letters, key = operator.itemgetter(0), reverse = False)\n",
    "\n",
    "        if return_stats == False:\n",
    "            if verbose == True:\n",
    "                # print(\"\\n------\\nStats:\")\n",
    "                # print(\"----------\")\n",
    "                print(f\"Letters in correct positions: {perf_letters}\")\n",
    "                print(f\"Letters in incorrect positions: {incorr_pos}\")\n",
    "                print(f\"Incorrect letters: {wrong_letters}\")\n",
    "                # print(\"----------\\n\")\n",
    "\n",
    "        potential_next_words = get_possible_guesses(word_list = word_list, perf_letters = perf_letters, incorr_pos = incorr_pos, wrong_letters = wrong_letters)\n",
    "        potential_next_words = [word for word in potential_next_words if word not in guessed_words] # excludes already guessed words, or else function runs infinitely\n",
    "\n",
    "        if return_stats == False:\n",
    "            if verbose == True:\n",
    "                time.sleep(drama / 1.5)\n",
    "                if len(potential_next_words) != 1:\n",
    "                    print(f\"\\n{len(potential_next_words)} possible words remaining.\\n\")\n",
    "                else:\n",
    "                    print(f\"\\n{len(potential_next_words)} possible word remaining.\\n\")\n",
    "\n",
    "        if len(potential_next_words) > opt_thres:\n",
    "            \n",
    "            if return_stats == False:\n",
    "                if verbose == True:\n",
    "                    print(\"------------------\")    \n",
    "                    time.sleep(drama / 1.5)\n",
    "                    print(f\"\\nNot enough related information in the previous guess.\\n\")\n",
    "                    time.sleep(drama)\n",
    "                    print(f\"Finding best next guess with new letters...\\n\")\n",
    "                    time.sleep(drama / 1.5)\n",
    "                    # print(\"------------------\")    \n",
    "\n",
    "            while ideal_letter_diversity:\n",
    "            \n",
    "                remaining_opposites = set()\n",
    "                for word in official_words:\n",
    "                    if word not in guessed_words: # eliminate previously guessed words\n",
    "                        if len(set(word).difference(set(all_guessed_letts))) == wordlen:\n",
    "                            remaining_opposites.add(word)\n",
    "\n",
    "                if len(remaining_opposites) > 0: # if it can find at least one word\n",
    "                    guess_ratings = get_word_meaning(words_to_rate = list(remaining_opposites), word_list = word_list, normalized = False, ascending = False)\n",
    "                    break\n",
    "                else:\n",
    "                    ideal_letter_diversity -= 1 # try with 4 new unique letters, then 3, then 2, etc\n",
    "                \n",
    "                if ideal_letter_diversity == 0: # if it can't find any words with at least 1 new letter (this should be impossible, but including this just in case, as a killswitch)\n",
    "                    guess_ratings = get_word_meaning(words_to_rate = potential_next_words, word_list = word_list, normalized = False, ascending = False)\n",
    "                    break\n",
    "                else: # \n",
    "                    guess_ratings = get_word_meaning(words_to_rate = list(remaining_opposites), word_list = word_list, normalized = False, ascending = False)\n",
    "\n",
    "        else: # if there are less than optim_thresh remaining possible words, only count remaining words as ones that satisfy all 3 of the checked criteria\n",
    "            \n",
    "            # checking for perfect letters\n",
    "            if len(perf_letters) > 0:\n",
    "                keeps = set()\n",
    "                for word in potential_next_words:\n",
    "                    for goodlett, goodpos in perf_letters:\n",
    "                        if word[goodpos] == goodlett: # word letters ARE those letters in those spots\n",
    "                            keeps.add(word)\n",
    "\n",
    "            # checking for words with letters in incorrect pos, adding them to exclusions set\n",
    "            if len(incor_pos) > 0:\n",
    "                excludes = set()\n",
    "                for word in potential_next_words:\n",
    "                    for badlett, badpos in incor_pos:\n",
    "                        if word[badpos] == badlett: \n",
    "                            excludes.add(word)\n",
    "\n",
    "            # checking for words with bad letters anywhere in them, adding them to exclusions set\n",
    "            if len(wrong_letters) > 0:\n",
    "                for word in potential_next_words:\n",
    "                    if len(set(word).difference(set(wrong_letters))) < len(set(word)): # if the difference is less than the number of unique chars in the word (there aren't always 5 unique chars)\n",
    "                        excludes.add(word)\n",
    "\n",
    "            potential_next_words = list(potential_next_words)\n",
    "            guess_ratings = get_word_meaning(words_to_rate = potential_next_words, word_list = word_list, normalized = False, ascending = False)\n",
    "\n",
    "        if intensifier == True:\n",
    "            guess_ratings = intensify_by_positions(word_list = official_words, ratings_list = guess_ratings, strength = mult_strength, rounding = 2)\n",
    "\n",
    "        words_remaining.append(len(guess_ratings))\n",
    "        guess = guess_ratings[0][0] # word in [(word, rating)] structure. Chooses first word in first tuple\n",
    "        avg_guess_meaning.append(guess_ratings[0][1])\n",
    "        if return_stats == False:\n",
    "            if verbose == True:\n",
    "                time.sleep(drama / 1.5)\n",
    "                print(f\"Next guess: '{guess}'\\n\")\n",
    "                print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "\n",
      "Guess 1: 'irate'\n",
      "\n",
      "------------------\n",
      "\n",
      "Guess 2: 'stole'\n",
      "\n",
      "------------------\n",
      "\n",
      "Guess 3: 'dunce'\n",
      "\n",
      "------------------\n",
      "\n",
      "Guess 4: 'thyme'\n",
      "\n",
      "------------------\n",
      "\n",
      "The puzzle was solved after 4 guesses.\n",
      "The target word was 'thyme'.\n",
      "\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NEXT STEPS:\n",
    "- add prefix/suffix bias (whichever is currently in ww1.0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "wordlist = official_words.copy() # so the list doesn't grow with each OOV word\n",
    "\n",
    "starting_word = \"irate\"\n",
    "target_word = \"thyme\"\n",
    "\n",
    "# starting_word = random.choice(official_words)\n",
    "# target_word = random.choice(official_words)\n",
    "\n",
    "wordle_wizard2(guess = starting_word, target = target_word, word_list = wordlist,\n",
    "               opt_thres = 100, max_guesses = 8, gram_bias = \"suffix\",\n",
    "               intensifier = False, mult_strength = 10,\n",
    "               return_stats = False, drama = 0.0, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arose', 38.02), ('adore', 35.72), ('opera', 35.49)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_guess_words(word_list = official_words, show_letters = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5124"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pairs = []\n",
    "\n",
    "for word1 in official_words:\n",
    "    for word2 in official_words:\n",
    "        if Levenshtein.distance(word1, word2) == 1:\n",
    "            word_pairs.append((word1, word2))\n",
    "\n",
    "len(list(set(word_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('share1', 15),\n",
       " ('store1', 13),\n",
       " ('stare1', 12),\n",
       " ('shore1', 12),\n",
       " ('shale1', 11),\n",
       " ('stale1', 11),\n",
       " ('stack1', 10),\n",
       " ('patty1', 10),\n",
       " ('cover1', 9),\n",
       " ('spare1', 9)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(word_pairs)\n",
    "# sorted(word_pairs, key = operator.itemgetter(1), reverse = False) \n",
    "ones_counts = {}\n",
    "for word1, word2 in word_pairs:\n",
    "    one = word1 + \"1\"\n",
    "    if one not in ones_counts:\n",
    "        ones_counts[one] = 1\n",
    "    else:\n",
    "        ones_counts[one] += 1\n",
    "\n",
    "sorted(ones_counts.items(), key = operator.itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snare\n",
      "stare\n",
      "scare\n",
      "shard\n",
      "spare\n",
      "shape\n",
      "shake\n",
      "shade\n",
      "shore\n",
      "shame\n",
      "shale\n",
      "shire\n",
      "sharp\n",
      "shave\n",
      "shark\n"
     ]
    }
   ],
   "source": [
    "for word in official_words:\n",
    "    if Levenshtein.distance(\"share\", word) == 1:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kmaurinjones/Desktop/data_science/data_science_projects/wordle_wizard_gh/wordle_wizard2.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kmaurinjones/Desktop/data_science/data_science_projects/wordle_wizard_gh/wordle_wizard2.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m asdf\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdf' is not defined"
     ]
    }
   ],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1679360070.10414\n",
      "1679360070.104507\n"
     ]
    }
   ],
   "source": [
    "len(official_words) // 10\n",
    "len(official_words) // 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('suffix', 'irate', 86.18), '0.04 seconds per bias')\n",
      "(('prefix', 'irate', 88.0), '0.04 seconds per bias')\n",
      "((None, 'irate', 83.24), '0.04 seconds per bias')\n",
      "(('suffix', 'fuzzy', 104.81), '0.05 seconds per bias')\n",
      "(('prefix', 'fuzzy', 108.68), '0.05 seconds per bias')\n",
      "((None, 'fuzzy', 98.44), '0.04 seconds per bias')\n",
      "13853 iterations run. 123 combinations excepted.\n",
      "(13853, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_guess</th>\n",
       "      <th>target</th>\n",
       "      <th>intensifier</th>\n",
       "      <th>mult_strength</th>\n",
       "      <th>opt_thresh</th>\n",
       "      <th>first_guess_vows</th>\n",
       "      <th>first_guess_cons</th>\n",
       "      <th>target_vows</th>\n",
       "      <th>target_cons</th>\n",
       "      <th>num_guesses</th>\n",
       "      <th>avg_guess_meaning</th>\n",
       "      <th>avg_vows_per_guess</th>\n",
       "      <th>avg_cons_per_guess</th>\n",
       "      <th>valid_solution</th>\n",
       "      <th>gram_bias</th>\n",
       "      <th>avg_similarity</th>\n",
       "      <th>luck</th>\n",
       "      <th>avg_words_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>irate</td>\n",
       "      <td>thyme</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>31.32</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>2.933333</td>\n",
       "      <td>False</td>\n",
       "      <td>suffix</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.061587</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>irate</td>\n",
       "      <td>mower</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>30.03</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>False</td>\n",
       "      <td>suffix</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.080952</td>\n",
       "      <td>30.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>irate</td>\n",
       "      <td>horde</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>29.49</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>2.933333</td>\n",
       "      <td>False</td>\n",
       "      <td>suffix</td>\n",
       "      <td>0.890476</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>irate</td>\n",
       "      <td>heard</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>31.08</td>\n",
       "      <td>2.933333</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>False</td>\n",
       "      <td>suffix</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>irate</td>\n",
       "      <td>tenor</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>31.44</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>suffix</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_guess target  intensifier  mult_strength  opt_thresh  \\\n",
       "0       irate  thyme        False             10         100   \n",
       "1       irate  mower        False             10         100   \n",
       "2       irate  horde        False             10         100   \n",
       "3       irate  heard        False             10         100   \n",
       "4       irate  tenor        False             10         100   \n",
       "\n",
       "   first_guess_vows  first_guess_cons  target_vows  target_cons  num_guesses  \\\n",
       "0                 3                 2            2            3           15   \n",
       "1                 3                 2            2            3            7   \n",
       "2                 3                 2            2            3           15   \n",
       "3                 3                 2            2            3           15   \n",
       "4                 3                 2            2            3            3   \n",
       "\n",
       "   avg_guess_meaning  avg_vows_per_guess  avg_cons_per_guess  valid_solution  \\\n",
       "0              31.32            2.066667            2.933333           False   \n",
       "1              30.03            2.285714            2.714286           False   \n",
       "2              29.49            2.066667            2.933333           False   \n",
       "3              31.08            2.933333            2.066667           False   \n",
       "4              31.44            2.333333            2.666667            True   \n",
       "\n",
       "  gram_bias  avg_similarity      luck  avg_words_remaining  \n",
       "0    suffix        0.923810  0.061587            57.000000  \n",
       "1    suffix        0.566667  0.080952            30.166667  \n",
       "2    suffix        0.890476  0.059365            49.000000  \n",
       "3    suffix        0.857143  0.057143            52.000000  \n",
       "4    suffix        0.000000  0.000000            84.500000  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "## Takes about 7min20s for 18.5k iterations\n",
    "\n",
    "excepts = [] # keeping track of word combinations that don't work for some reason\n",
    "\n",
    "stats_master = {}\n",
    "\n",
    "for start_word in [\"irate\", \"fuzzy\"]:\n",
    "    \n",
    "\n",
    "    for bias in [\"suffix\", \"prefix\", None]:\n",
    "        # for timing iterations\n",
    "        start = time.time()\n",
    "\n",
    "        for target_word in official_words:\n",
    "\n",
    "            try:\n",
    "                complete = wordle_wizard2(guess = start_word, target = target_word, word_list = official_words,\n",
    "                opt_thres = 100, max_guesses = 15, gram_bias = bias,\n",
    "                intensifier = False, mult_strength = 10,\n",
    "                return_stats = True, drama = 0.0, verbose = False)\n",
    "\n",
    "            except:\n",
    "                ZeroDivisionError\n",
    "                excepts.append((complete[\"first_guess\"], complete[\"target\"]))\n",
    "                \n",
    "            for metric, result in complete.items():\n",
    "                if metric in stats_master.keys():\n",
    "                    stats_master[metric].append(result)\n",
    "                else:\n",
    "                    stats_master[metric] = []\n",
    "\n",
    "        # for timing iterations\n",
    "        stop = time.time()\n",
    "        print(((bias, start_word, round(stop - start, 2)), f\"{round((stop - start) / len(official_words), 2)} seconds per bias\"))     \n",
    "\n",
    "### df creation and csv writing \n",
    "sims_df = pd.DataFrame(stats_master)\n",
    "\n",
    "print(f\"{len(sims_df)} iterations run. {len(excepts)} combinations excepted.\")\n",
    "# print(excepts[:10])\n",
    "# print(sims_df['first_guess'].unique().tolist())\n",
    "\n",
    "print(sims_df.shape)\n",
    "sims_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "('irate', 'suffix')\n",
      "nan\n",
      "------\n",
      "('irate', 'prefix')\n",
      "nan\n",
      "------\n",
      "('irate', 'None')\n",
      "nan\n",
      "------\n",
      "('fuzzy', 'suffix')\n",
      "nan\n",
      "------\n",
      "('fuzzy', 'prefix')\n",
      "nan\n",
      "------\n",
      "('fuzzy', 'None')\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# print(sims_df.query(\"opt_thres == 230\")['num_guesses'].mean()) # 4.28\n",
    "# print(sims_df.query(\"opt_thres == 23\")['num_guesses'].mean()) # 4.30\n",
    "for word in sims_df[\"first_guess\"].unique().tolist(): \n",
    "    for bias in sims_df[\"gram_bias\"].unique().tolist(): \n",
    "        print(\"------\")\n",
    "        print((word, bias))\n",
    "        print(sims_df.query(f\"first_guess == '{word}' & 'gram_bias' == '{bias}'\")['num_guesses'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing wordle_wizard2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[round(i, 1) for i in np.arange(1, 2, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'False: {sims_df.query(\"multiplier == False\")[\"num_guesses\"].mean()}')\n",
    "# print(f'False: {sims_df.query(\"multiplier == True\")[\"num_guesses\"].mean()}')\n",
    "print(sims_df.query(\"first_guess == 'later' & multiplier == True\")[\"num_guesses\"].mean())\n",
    "print(sims_df.query(\"first_guess == 'later' & multiplier == False\")[\"num_guesses\"].mean())\n",
    "print(\"-----\")\n",
    "print(sims_df.query(\"first_guess == 'fuzzy' & multiplier == True\")[\"num_guesses\"].mean())\n",
    "print(sims_df.query(\"first_guess == 'fuzzy' & multiplier == False\")[\"num_guesses\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_positional_dists(word_list = [\"tests\"], rating_method=\"ranked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_entropy(word: str, return_prob: bool = True):\n",
    "\n",
    "    char_freqs = {}\n",
    "\n",
    "    for char in word:\n",
    "        if char not in char_freqs:\n",
    "            char_freqs[char] = 1\n",
    "        else:\n",
    "            char_freqs[char] += 1\n",
    "\n",
    "    if return_prob == True:\n",
    "        prob_dist = []\n",
    "        for char, freq in char_freqs.items():\n",
    "            prob_dist.append((char, (freq/len(char_freqs))))        \n",
    "        sorted_freqs = sorted(prob_dist, key = operator.itemgetter(1), reverse = True)\n",
    "    else:\n",
    "        sorted_freqs = sorted(char_freqs.items(), key = operator.itemgetter(1), reverse = True)\n",
    "\n",
    "    return sorted_freqs\n",
    "\n",
    "result = get_word_entropy(\"abcdefghijklmnopqrstuvwxyz\", return_prob = True)\n",
    "result_letters = [letter for letter, prob in result]\n",
    "result_dist = [round(prob, 2) for letter, prob in result]\n",
    "print(result_letters)\n",
    "print(result_dist)\n",
    "\n",
    "from scipy.stats import entropy\n",
    "entropy(pk = result_dist) # only measures how diverse a word is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Best and Worst Words Against all Wordle Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excepts = [] # keeping track of word combinations that don't work for some reason\n",
    "\n",
    "stats_master = {}\n",
    "\n",
    "for start_word in [\"later\", \"fuzzy\"]:\n",
    "\n",
    "    for target_word in official_words:\n",
    "\n",
    "        ## only run the combination if it hasn't been done already\n",
    "        try:\n",
    "            complete = wordle_wizard(word_list = official_words, max_guesses = 15, \n",
    "                guess = start_word, target = target_word,\n",
    "                random_guess = False, random_target = False, \n",
    "                    verbose = False, drama = 0, return_stats = True)\n",
    "\n",
    "        except:\n",
    "            IndexError\n",
    "            excepts.append((complete[\"first_guess\"], complete[\"target_word\"]))\n",
    "            \n",
    "        for metric, result in complete.items():\n",
    "            if metric in stats_master.keys():\n",
    "                stats_master[metric].append(result)\n",
    "            else:\n",
    "                stats_master[metric] = []\n",
    "\n",
    "### df creation and csv writing \n",
    "sims_df = pd.DataFrame(stats_master)\n",
    "\n",
    "print(f\"{len(sims_df)} iterations run. {len(excepts)} combinations excepted.\")\n",
    "print(excepts[:10])\n",
    "\n",
    "print(sims_df.shape)\n",
    "print(sims_df['first_guess'].unique().tolist())\n",
    "sims_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "later_guesses = sims_df.query(\"first_guess == 'later'\")[['first_guess', 'num_guesses']]\n",
    "sims_df.query(\"first_guess == 'later'\")['num_guesses'].mean() # later : ~3.81 avg guesses\n",
    "\n",
    "for word in sims_df[\"first_guess\"].unique().tolist():\n",
    "    mean_guesses = round(sims_df.query(f\"first_guess == '{word}'\")['num_guesses'].mean(), 2) # later : ~3.81 avg guesses, fuzzy : ~4.36 avg guesses\n",
    "    word_df = sims_df.query(f\"first_guess == '{word}'\")['num_guesses']\n",
    "    \n",
    "    word_df_dist_plot = px.histogram(word_df, x = \"num_guesses\", title = f\"Distribution of Guesses with '{word}' as Starting Word\",\n",
    "                                        labels = {\"num_guesses\": \"Number of Guesses\"})\n",
    "    word_df_dist_plot.add_vline(x = mean_guesses, line_width = 4, line_dash = \"dot\", line_color = \"black\",\n",
    "                                    annotation_text = f\"Mean = {mean_guesses}\", annotation_font_size = 20,\n",
    "                                    annotation_font_color = \"black\", annotation_position = \"right\")\n",
    "    word_df_dist_plot.update_layout(title_font_size = 22)\n",
    "    word_df_dist_plot.update_traces(marker_color = \"#6ca965\")\n",
    "\n",
    "    word_df_dist_plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
